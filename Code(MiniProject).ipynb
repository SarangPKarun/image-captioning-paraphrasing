{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quOa5M4owCU7"
      },
      "source": [
        "# Load drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2shP18cq4u_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJM4RlvywGIj"
      },
      "source": [
        "# Step 1 — Importing required libraries for Image Captioning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the sentencepiece library, often used for text tokenization\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "f-jGxeZc7K2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzIQ2zTxrkux"
      },
      "outputs": [],
      "source": [
        "# Install the sentencepiece library, often used for text tokenization\n",
        "!pip install sentencepiece\n",
        "\n",
        "# Import necessary libraries\n",
        "import os  # For operating system operations like file handling\n",
        "import pickle  # For serializing and deserializing Python objects\n",
        "import numpy as np  # For numerical computations\n",
        "from tqdm.notebook import tqdm  # For displaying progress bars in notebooks\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import matplotlib.pyplot as plt  # For data visualization\n",
        "from textwrap import wrap  # For wrapping text\n",
        "import cv2  # OpenCV library for image processing\n",
        "import re  # For regular expressions\n",
        "import random  # For generating random numbers\n",
        "from PIL import Image, ImageFilter  # For image processing using the Pillow library\n",
        "\n",
        "# Import TensorFlow and Keras libraries for deep learning\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # For text tokenization\n",
        "from keras import applications  # For accessing pre-trained models and other utilities\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array  # For loading and converting images\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7, preprocess_input  # For using EfficientNetB7 model and preprocessing\n",
        "\n",
        "# Import additional Keras utilities for processing sequences and categorical data\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For padding sequences to the same length\n",
        "from tensorflow.keras.utils import to_categorical, plot_model  # For converting labels to categorical and plotting the model architecture\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, Reshape, concatenate  # For building neural network layers\n",
        "from tensorflow.keras.models import Model  # For creating a Keras model\n",
        "from keras import callbacks  # For using callbacks during training\n",
        "from keras.models import Sequential  # For creating a sequential Keras model\n",
        "from tensorflow.keras.models import load_model  # For loading a saved Keras model\n",
        "from keras.layers import Bidirectional  # For creating bidirectional LSTM layers\n",
        "\n",
        "# The following sections would typically include:\n",
        "# - Data loading and preprocessing\n",
        "# - Model architecture definition\n",
        "# - Compilation and training of the model\n",
        "# - Evaluation and inference steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRdgWYUkgtl"
      },
      "source": [
        "# Step 2 - Visualization of flickr dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-KYwLErkrPP"
      },
      "outputs": [],
      "source": [
        "# path to the image dataset\n",
        "img_dir='/content/drive/MyDrive/Mini Project/data/Images'\n",
        "# path to the caption dataset\n",
        "cap_dir='/content/drive/MyDrive/Mini Project/youtube/captions.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjALnov4k8I0"
      },
      "outputs": [],
      "source": [
        "def readImage(path):\n",
        "    \"\"\"\n",
        "    Reads an image from the specified path, resizes it to 250x250 pixels,\n",
        "    converts it to an array, and normalizes the pixel values to the range [0, 1].\n",
        "\n",
        "    Args:\n",
        "    path (str): The path to the image file.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: The processed image array.\n",
        "    \"\"\"\n",
        "    # Load the image with the specified color mode and target size\n",
        "    img = load_img(path, color_mode='rgb', target_size=(250, 250))\n",
        "    # Convert the image to an array\n",
        "    img = img_to_array(img)\n",
        "    # Normalize the image array to [0, 1] range\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "def display_images(temp_df):\n",
        "    \"\"\"\n",
        "    Displays a grid of 15 images and their corresponding captions from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    temp_df (pandas.DataFrame): A DataFrame containing image filenames and captions.\n",
        "    \"\"\"\n",
        "    # Reset the index of the DataFrame\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    # Set the figure size for the plot\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    n = 0  # Initialize the subplot index\n",
        "    # Loop through the first 15 images in the DataFrame\n",
        "    for i in range(15):\n",
        "        n += 1  # Increment the subplot index\n",
        "        # Create a subplot\n",
        "        plt.subplot(5, 5, n)\n",
        "        # Adjust the spacing between subplots\n",
        "        plt.subplots_adjust(hspace=0.7, wspace=0.3)\n",
        "        # Read and process the image\n",
        "        image = readImage(img_dir + '/' + temp_df.image[i])\n",
        "        # Display the image\n",
        "        plt.imshow(image)\n",
        "        # Display the image caption with word wrapping\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n",
        "        # Hide the axis\n",
        "        plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is6D6FwPlGvV"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file containing image captions into a DataFrame\n",
        "captionlist = pd.read_csv(cap_dir)\n",
        "\n",
        "# Sample 15 random entries from the DataFrame and display the corresponding images and captions\n",
        "display_images(captionlist.sample(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpl3swy9RWmp"
      },
      "outputs": [],
      "source": [
        "# now save this training feature to drive\n",
        "print('length of feature vectors for training : ',len(features))\n",
        "\n",
        "#saving in drive\n",
        "print('saving.....')\n",
        "pickle.dump(features, open('/content/drive/MyDrive/Mini Project/minorproject/vgg16placeshybridfeatures.pkl', 'wb'))\n",
        "print('Shape of a feature vector : ',features['2513260012_03d33305cf'].shape)\n",
        "print('saved!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUf-A_SAoo1R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsajfzrdKeKr"
      },
      "source": [
        "# Step 3 — Extract features from the images using EfficientNetB7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGZqC0D7wvMB"
      },
      "outputs": [],
      "source": [
        "# No of images in the dataset\n",
        "print(len(os.listdir('/content/drive/MyDrive/Mini Project/data/Images')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpgcEfrNKeKr"
      },
      "outputs": [],
      "source": [
        "# EfficientNetB7 is used to extract features from the image; the fully connected layer is not used (include_top=False) to get features only\n",
        "model = EfficientNetB7(weights='imagenet', include_top=False)\n",
        "\n",
        "# Restructure the model to output the features from the desired layer (last convolutional layer)\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# Freeze the layers to prevent them from being trainable\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Print the model summary to see the architecture and layers\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B6BtAzIsNuA2"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to your image\n",
        "img_path = '/content/drive/MyDrive/Mini Project/data/Images/2513260012_03d33305cf.jpg'\n",
        "\n",
        "# Load the image with bicubic interpolation using Keras\n",
        "img = load_img(img_path, target_size=(600, 600), interpolation='lanczos')\n",
        "\n",
        "# Convert the image to a NumPy array\n",
        "img_array = img_to_array(img)\n",
        "\n",
        "# Convert the NumPy array to PIL Image\n",
        "pil_img = Image.fromarray(img_array.astype('uint8'))\n",
        "\n",
        "# Apply edge enhancement filter on the image\n",
        "enhanced_image = pil_img.filter(ImageFilter.EDGE_ENHANCE)\n",
        "\n",
        "# Convert the enhanced image back to NumPy array\n",
        "enhanced_img_array = img_to_array(enhanced_image)\n",
        "\n",
        "# Display the original and enhanced images\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_array / 255.0)  # Normalize pixel values for display\n",
        "plt.title('Original Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(enhanced_img_array / 255.0)  # Normalize pixel values for display\n",
        "plt.title('Enhanced Image (Edge Enhancement)')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ORP7k1dKxAT5"
      },
      "outputs": [],
      "source": [
        "# to show the resize image. demo purpose only\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to your image\n",
        "img_path = '/content/drive/MyDrive/Mini Project/data/Images/2513260012_03d33305cf.jpg'\n",
        "\n",
        "# Load the original image\n",
        "img2 = load_img(img_path)\n",
        "\n",
        "# Load the image with bicubic interpolation and resize to 600x600 pixels\n",
        "img = load_img(img_path, target_size=(600, 600), interpolation='lanczos')\n",
        "\n",
        "# Convert the resized image to a NumPy array\n",
        "img_array = img_to_array(img)\n",
        "\n",
        "# Display the original and resized images\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img2)\n",
        "plt.title('Original Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(img_array / 255.0)  # Normalize pixel values for display\n",
        "plt.title('Resized Image (Bicubic Interpolation)')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmAUwAe4KeKs"
      },
      "outputs": [],
      "source": [
        "# extract features from image\n",
        "features = {}\n",
        "directory = '/content/drive/MyDrive/Mini Project/data/Images'\n",
        "\n",
        "# Iterate through each image in the directory\n",
        "for img_name in tqdm(os.listdir(directory)):\n",
        "    # Load the image from file\n",
        "    img_path = directory + '/' + img_name\n",
        "    image = load_img(img_path, target_size=(600, 600), interpolation='lanczos')\n",
        "\n",
        "    # Convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "\n",
        "    # Convert the NumPy array to PIL Image\n",
        "    pil_img = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "    # Apply edge enhancement filter on the image\n",
        "    enhanced_image = pil_img.filter(ImageFilter.EDGE_ENHANCE)\n",
        "\n",
        "    # Convert the enhanced image back to NumPy array\n",
        "    image = img_to_array(enhanced_image)\n",
        "\n",
        "    # Reshape data for model input\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "    # Preprocess image for EfficientNetB7\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    # Extract features using the pre-trained EfficientNetB7 model\n",
        "    feature = model.predict(image, verbose=0)\n",
        "\n",
        "    # Get image ID from the filename (remove the extension)\n",
        "    image_id = img_name.split('.')[0]\n",
        "\n",
        "    # Store extracted feature in the dictionary\n",
        "    features[image_id] = feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPF3ijMJKeKt"
      },
      "outputs": [],
      "source": [
        "# Print the length of the feature vectors for training\n",
        "print('length of feature vectors for training : ', len(features))\n",
        "\n",
        "# Saving the feature vectors to Google Drive\n",
        "print('saving.....')\n",
        "pickle.dump(features, open('/content/drive/MyDrive/Mini Project/minorproject/edgeehancedefficient.pkl', 'wb'))\n",
        "\n",
        "# Print the shape of a specific feature vector for verification\n",
        "print('Shape of a feature vector : ', features['2513260012_03d33305cf'].shape)\n",
        "\n",
        "# Confirm the features have been saved\n",
        "print('saved!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZP5S46Fxocl"
      },
      "source": [
        "# Step 4 — Load the feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7I0rCb1KeKt"
      },
      "outputs": [],
      "source": [
        "# Load features from a pickle file\n",
        "with open('/content/drive/MyDrive/Mini Project/efficientnetb7/allextractedfeature.pkl', 'rb') as f:\n",
        "    features = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbzay-ckxtSK"
      },
      "outputs": [],
      "source": [
        "# Print the shape of a specific feature vector\n",
        "print('Shape of a feature vector : ', features['2513260012_03d33305cf'].shape)\n",
        "\n",
        "# Print the total number of images (keys) in the features dictionary\n",
        "print(\"Total images : \", len(features.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip Step 5 and step 6 if paraphrased already."
      ],
      "metadata": {
        "id": "tMQkBDNhFH0j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih394hMVKeKu"
      },
      "source": [
        "# Step 5 — Load descriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only need to run Step 5 once, then run Step 6 to paraphrase."
      ],
      "metadata": {
        "id": "bezXyWGtFzuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvOdONQtKeKu"
      },
      "outputs": [],
      "source": [
        "# Open and read the contents of the captions.txt file located in Google Drive\n",
        "with open(os.path.join('/content/drive/MyDrive/Mini Project/youtube/captions.txt'), 'r') as f:\n",
        "    next(f)  # Skip the first line\n",
        "    captions_doc = f.read()\n",
        "\n",
        "# Print the contents of the captions document\n",
        "print(captions_doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfK3YEF3KeKv"
      },
      "outputs": [],
      "source": [
        "# Create a mapping of image to captions\n",
        "mapping = {}\n",
        "\n",
        "# Process each line in the captions document\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    # Split the line by comma(,)\n",
        "    tokens = line.split(',')\n",
        "\n",
        "    # Skip lines with fewer than 2 tokens\n",
        "    if len(tokens) < 2:\n",
        "        continue\n",
        "\n",
        "    # Extract image ID and captions\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "\n",
        "    # Remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "\n",
        "    # Convert caption list to string\n",
        "    caption = \" \".join(caption)\n",
        "\n",
        "    # Create a list in the mapping dictionary if needed\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "\n",
        "    # Store the caption\n",
        "    mapping[image_id].append(caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbDkIDMUKeKv"
      },
      "outputs": [],
      "source": [
        "len(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before cleaning : \\n')\n",
        "mapping['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "id": "xMrQy8O9s8t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_h9yZBRlsg5"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Mini Project/minorproject/paramapping.pkl', 'wb') as f:\n",
        "    # Pickle the dictionary\n",
        "    pickle.dump(mapping, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daJQBf2bxz7m"
      },
      "source": [
        "# Step 6 — Paraphrase sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVr4Lm-Pl7op"
      },
      "outputs": [],
      "source": [
        "# Open the file in binary read mode\n",
        "with open('/content/drive/MyDrive/Mini Project/minorproject/paramapping.pkl', 'rb') as f:\n",
        "    # Unpickle the dictionary\n",
        "    paramapping = pickle.load(f)\n",
        "\n",
        "# Print the dictionary\n",
        "print(paramapping['1000268201_693b08cb0e'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LJEJuPz2BdU"
      },
      "outputs": [],
      "source": [
        "# Initialize a counter for images with 10 descriptions\n",
        "i = 0\n",
        "\n",
        "# Iterate through each image ID and its descriptions in paramapping\n",
        "for image_id, descriptions in tqdm(paramapping.items()):\n",
        "    # Check if the image has exactly 10 descriptions\n",
        "    if len(descriptions) == 10:\n",
        "        i += 1\n",
        "\n",
        "# Print the count of images with 10 descriptions\n",
        "print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOwdEFiDyRfu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "def get_response(input_text):\n",
        "    # Tokenize the input text\n",
        "    batch = tokenizer([input_text], truncation=True, padding='longest', max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "\n",
        "    # Generate paraphrased text\n",
        "    translated = model.generate(**batch, max_length=60, num_beams=1, num_return_sequences=1, temperature=1.5)\n",
        "\n",
        "    # Decode the generated tokens to text\n",
        "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "\n",
        "    return tgt_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R39eEjjaaid"
      },
      "outputs": [],
      "source": [
        "def paraphrase_and_yield(mapping):\n",
        "    # Iterate through each image ID and its descriptions in the mapping\n",
        "    for image_id, descriptions in tqdm(mapping.items()):\n",
        "        # Check if there are exactly 10 descriptions\n",
        "        if len(descriptions) != 10:\n",
        "            # Paraphrase each description using get_response()\n",
        "            paraphrased_descriptions = [get_response(description) for description in descriptions]\n",
        "\n",
        "            # Flatten the list of paraphrased descriptions\n",
        "            flat_list = []\n",
        "            for sublist in paraphrased_descriptions:\n",
        "                flat_list.extend(sublist)\n",
        "\n",
        "            # Extend the original descriptions with the paraphrased versions\n",
        "            mapping[image_id].extend(flat_list)\n",
        "\n",
        "            # Yield the updated mapping\n",
        "            yield mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uC0UxCLvJxK"
      },
      "outputs": [],
      "source": [
        "# Iterate through the updated mappings yielded by paraphrase_and_yield(paramapping)\n",
        "for updated_mapping in paraphrase_and_yield(paramapping):\n",
        "    # Save each updated_mapping as a pickle file\n",
        "    with open('/content/drive/MyDrive/Mini Project/minorproject/paramapping.pkl', 'wb') as f:\n",
        "        pickle.dump(updated_mapping, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i34QhMXKeKw"
      },
      "source": [
        "# Step 7 — Clean and Save image descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qj9YqWsfPn3"
      },
      "outputs": [],
      "source": [
        "# Open the file in binary read mode\n",
        "with open('/content/drive/MyDrive/Mini Project/minorproject/paramapping.pkl', 'rb') as f:\n",
        "    # Unpickle the dictionary\n",
        "    mapping = pickle.load(f)\n",
        "\n",
        "# Print the dictionary\n",
        "print(mapping['1000268201_693b08cb0e'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z7-fHAnKeKw"
      },
      "outputs": [],
      "source": [
        "# function to clean the description\n",
        "\n",
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # preprocessing steps\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # delete digits, special chars, etc.,\n",
        "            caption = re.sub('[^A-Za-z ]', '', caption)\n",
        "            # delete additional spaces\n",
        "            caption = re.sub('\\s+', ' ', caption).strip()\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            captions[i] = caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtYgLVvY8_mc"
      },
      "outputs": [],
      "source": [
        "print('Before cleaning : \\n')\n",
        "mapping['1000268201_693b08cb0e']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjvIzjYMKeKz"
      },
      "outputs": [],
      "source": [
        "# preprocess the text\n",
        "clean(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPjcduWTKeK0"
      },
      "outputs": [],
      "source": [
        "print('After cleaning : \\n')\n",
        "\n",
        "mapping['1000268201_693b08cb0e']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLLklcvm2Noa"
      },
      "source": [
        "# Step 8 -Tokenize the caption (All caption are tokenized fully)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcfZF7-tKeK1"
      },
      "outputs": [],
      "source": [
        "all_captions = []\n",
        "\n",
        "# Iterate through each key in the mapping dictionary\n",
        "for key in mapping:\n",
        "    # Iterate through each caption in the list associated with the current key\n",
        "    for caption in mapping[key]:\n",
        "        # Append the caption to the all_captions list\n",
        "        all_captions.append(caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcDs1EXDKeK1"
      },
      "outputs": [],
      "source": [
        "len(all_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t8NjMp_KeK2"
      },
      "outputs": [],
      "source": [
        "all_captions[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWbj0DPnKeK2"
      },
      "outputs": [],
      "source": [
        "# tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzQvh-nR9xGr"
      },
      "outputs": [],
      "source": [
        "def search_sentences(sentences, target_word):\n",
        "    matching_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if target_word.lower() in sentence.lower():\n",
        "            matching_sentences.append(sentence)\n",
        "    return matching_sentences\n",
        "\n",
        "# Example usage with all_captions and target_word\n",
        "target_word = \"four\"\n",
        "\n",
        "matching_sentences = search_sentences(all_captions, target_word)\n",
        "\n",
        "# Print the matching sentences\n",
        "print(f\"Sentences containing the word '{target_word}':\")\n",
        "for sentence in matching_sentences:\n",
        "    print(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORtYyUWb8vO4"
      },
      "outputs": [],
      "source": [
        "'seven' in tokenizer.word_index.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6fJiJA6KeK2"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHuMqU26KeK3"
      },
      "outputs": [],
      "source": [
        "# Calculate the maximum length of captions in terms of number of words\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "\n",
        "# Print the maximum length\n",
        "print(max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W0t5PV-wejd"
      },
      "outputs": [],
      "source": [
        "# count the words\n",
        "# Get the word counts\n",
        "word_counts = tokenizer.word_counts\n",
        "\n",
        "# Convert to DataFrame\n",
        "word_counts_df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n",
        "\n",
        "# Sort the DataFrame by count in descending order\n",
        "word_counts_df = word_counts_df.sort_values(by='Count', ascending=False)\n",
        "# Reset the index to keep the original index\n",
        "word_counts_df = word_counts_df.reset_index(drop=True)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(word_counts_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVx9fYYQx5n8"
      },
      "outputs": [],
      "source": [
        "# display the count of the words\n",
        "\n",
        "topn = 50\n",
        "\n",
        "\n",
        "# Function to plot histogram\n",
        "def plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n",
        "    plt.figure(figsize=(30,3))\n",
        "    plt.bar(dfsub.index,dfsub['Count'],color ='r')\n",
        "    plt.yticks(fontsize=20,color ='b')\n",
        "    plt.xticks(dfsub.index,dfsub[\"Word\"],rotation=90,fontsize=20,color ='b')\n",
        "    plt.title(title,fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot the top 50 most frequent words\n",
        "plthist(word_counts_df.iloc[:topn,:],\n",
        "        title=\"The top 50 most frequently appearing words\")\n",
        "\n",
        "# Plot the least 50 most frequent words\n",
        "plthist(word_counts_df.iloc[-topn:,:],\n",
        "        title=\"The least 50 most frequently appearing words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJHPsSv7xvCy"
      },
      "source": [
        "#Step 9 - Embeddding word with Glove.6b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G40CmM05yscP"
      },
      "outputs": [],
      "source": [
        "tokenizer.word_index['car']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8OdHn4SzMpT"
      },
      "outputs": [],
      "source": [
        "# emb_mat[119]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ4-7cl9x9Tb"
      },
      "outputs": [],
      "source": [
        "emb_dim=200\n",
        "#len(word_map)=9385\n",
        "emb_mat= np.zeros((vocab_size,emb_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNhQ_u6H09aR"
      },
      "outputs": [],
      "source": [
        "emb_mat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YAM_fdlxxri"
      },
      "outputs": [],
      "source": [
        "# Load GloVe embeddings and populate emb_mat for words in tokenizer's word_index\n",
        "with open('/content/drive/MyDrive/Mini Project/data/glove.6B.200d.txt') as f:\n",
        "        for line in f:\n",
        "          word, *emb = line.split()\n",
        "          if word in tokenizer.word_index.keys():\n",
        "            emb_mat[tokenizer.word_index[word]]=np.array(emb,dtype=\"float32\")[:emb_dim]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_Ba3B3KeK3"
      },
      "source": [
        "# Step 10 - Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpDsLhxo-A-g"
      },
      "outputs": [],
      "source": [
        "type(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhBPLtOvKeK3"
      },
      "outputs": [],
      "source": [
        "# Get list of image_ids from mapping dictionary keys\n",
        "image_ids = list(mapping.keys())\n",
        "\n",
        "# Define the split ratio\n",
        "split_ratio = 0.889878\n",
        "\n",
        "# Calculate the index to split the data\n",
        "split_index = int(len(image_ids) * split_ratio)\n",
        "\n",
        "# Split image_ids into training and test sets\n",
        "train = image_ids[:split_index]\n",
        "test = image_ids[split_index:]\n",
        "\n",
        "# Print the lengths of the training and test sets\n",
        "print(\"Length of test set:\", len(test))\n",
        "print(\"Length of train set:\", len(train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7viRjaeOBa6A"
      },
      "outputs": [],
      "source": [
        "train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgLdOhFa0kdZ"
      },
      "source": [
        "# Step 11 — Data genertor with batch size to make it less memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX7khWVZKeK4"
      },
      "outputs": [],
      "source": [
        "# '<start> girl going into wooden building <end>'\n",
        "# X                                                      y\n",
        "# <start>                                                girl\n",
        "# <start> girl                                           going\n",
        "# <start> girl going                                     into\n",
        "# <start> girl going into                                wooden\n",
        "# <start> girl going into wooden                         building\n",
        "# <start> girl going into wooden building                <end>\n",
        "# <start> girl going into wooden building <end>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W404odgdmlln"
      },
      "outputs": [],
      "source": [
        "# JUST FOR CHECKING?\n",
        "cap=mapping['386656845_4e77c3e3da'][0]\n",
        "print(cap)\n",
        "seq=tokenizer.texts_to_sequences([cap])[0]\n",
        "print(seq, '\\n\\n')\n",
        "for i in range(1, len(seq)):\n",
        "  #  split into input and output pairs\n",
        "  in_seq, out_seq = seq[:i], seq[i]\n",
        "  # pad input sequence\n",
        "  in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "  print('in_seq : ',in_seq)\n",
        "  # # encode out sequence\n",
        "  out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
        "  print('out_seq : ',len(out_seq)) #here we use to_categorical to make one hot encoding of output having size of vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AN-uE6PudaZ"
      },
      "outputs": [],
      "source": [
        "type(features[train[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMmk7vX8KeK4"
      },
      "outputs": [],
      "source": [
        "# create data generator to get data in batch (avoids session crash)\n",
        "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    # loop over images\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = mapping[key]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # split the sequence into X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pairs\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "                    # store the sequences\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "            # Yield the batch data if batch size is reached\n",
        "            if n == batch_size:\n",
        "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "                yield [X1, X2], y\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTXEsGSjKeK4"
      },
      "source": [
        "# Step 12 - Model Creation and saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uJkf9QhDdZm"
      },
      "outputs": [],
      "source": [
        "# Image input\n",
        "inputs1 = Input(shape=(2560,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "fe3 = Reshape((1, 256))(fe2)  # Reshape for concatenation later\n",
        "\n",
        "# Text input\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "emb1 = Embedding(input_dim=vocab_size, output_dim=emb_dim, weights=[emb_mat], trainable=False)(inputs2)\n",
        "dr1 = Dropout(0.2)(emb1)\n",
        "lstm1 = LSTM(128, return_sequences=True)(dr1)\n",
        "dr2 = Dropout(0.2)(lstm1)\n",
        "lstm2 = LSTM(256, return_sequences=False)(dr2)  # Only need final state for concatenation\n",
        "\n",
        "# Concatenate image and text features\n",
        "concatenated = concatenate([fe3, lstm2], axis=1)\n",
        "\n",
        "# Additional layers\n",
        "conc1 = LSTM(256)(concatenated)\n",
        "conc2 = Dropout(0.4)(conc1)\n",
        "\n",
        "# Decoder layers\n",
        "decoder1 = add([fe2, conc2])  # Skip connection\n",
        "decoder2 = Dense(1000, activation='relu')(decoder1)\n",
        "output = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Plot the model architecture\n",
        "plot_model(model, show_shapes=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hDZR9PrKeK5"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "epochs = 20\n",
        "batch_size = 41\n",
        "steps = len(train) // batch_size\n",
        "\n",
        "for i in range(epochs):\n",
        "    # create data generator\n",
        "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "    # fit for one epoch\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qq5unblKeK5"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "model.save('/content/drive/MyDrive/Mini Project/minorproject'+'/bluescore90highest.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EloMTiZUNGYQ"
      },
      "outputs": [],
      "source": [
        "model.save_weights('/content/drive/MyDrive/Mini Project/minorproject'+'/bluescore90highestweight.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB1LPrneKeK6"
      },
      "source": [
        "# Step 13 - Load the final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjkusTnYr-xS"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model=load_model('/content/drive/MyDrive/Mini Project/minorproject'+'/bluescore90highest.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o45rpmURNQG9"
      },
      "outputs": [],
      "source": [
        "model.load_weights('/content/drive/MyDrive/Mini Project/minorproject'+'/bluescore90highestweight.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwoSxFWgz73P"
      },
      "source": [
        "# Step 14 - Generate Captions for the Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxshWGJJKeK6"
      },
      "outputs": [],
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-1N4x8RKeK6"
      },
      "outputs": [],
      "source": [
        "# generate caption for an image\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR4vhTRDKeK7"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# validate with test data\n",
        "actual, predicted = list(), list()\n",
        "\n",
        "for key in tqdm(test):\n",
        "    # get actual caption\n",
        "    captions = mapping[key]\n",
        "    # predict the caption for image\n",
        "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
        "    # split into words\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    y_pred = y_pred.split()\n",
        "    # append to the list\n",
        "    actual.append(actual_captions)\n",
        "    predicted.append(y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOMJmzJQxUYX"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# validate with test data\n",
        "actual, predicted = list(), list()\n",
        "\n",
        "for key in tqdm(test):\n",
        "    # get actual caption\n",
        "    captions = mapping[key]\n",
        "    # split into words\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    # append to the list\n",
        "    actual.append(actual_captions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8FU7pCUn3_t"
      },
      "outputs": [],
      "source": [
        "# to take only 5 captions for testing\n",
        "result = [\n",
        "    list_of_lists[::2]\n",
        "    for list_of_lists in actual\n",
        "]\n",
        "\n",
        "# Printing the result\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ew5Q5q7rpqE"
      },
      "outputs": [],
      "source": [
        "# to make list into sentences\n",
        "\n",
        "actualsentences = [\n",
        "    [\n",
        "        ' '.join(inner_list)\n",
        "        for inner_list in outer_list\n",
        "    ]\n",
        "    for outer_list in result\n",
        "]\n",
        "\n",
        "predictedsentences = [\n",
        "        ' '.join(inner_list)\n",
        "    for inner_list in predicted\n",
        "]\n",
        "\n",
        "# Printing the sentences\n",
        "print(actualsentences)\n",
        "print(predictedsentences)\n",
        "print(len(actualsentences))\n",
        "print(len(predictedsentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkByc41spPhh"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# from nltk.translate.glue_score import corpus_glue\n",
        "from nltk.translate.chrf_score import corpus_chrf\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "print(\"The cumulative and individual 1-gram BLEU use the same weights, e.g. (1, 0, 0, 0)\")\n",
        "# print('Sentence Bleu 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
        "print('Corpus Bleu 1-gram: %f' % corpus_bleu(result, predicted, weights=(1, 0, 0, 0)))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"The cumulative and individual 2-gram BLEU use the same weights, e.g. (0, 1, 0, 0)\")\n",
        "# print('Sentence Bleu 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
        "print('Corpus Bleu 2-gram: %f' % corpus_bleu(result, predicted, weights=(0, 1, 0, 0)))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"The cumulative and individual 3-gram BLEU use the same weights, e.g. (0, 0, 1, 0)\")\n",
        "# print('Sentence Bleu 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
        "print('Corpus Bleu 3-gram: %f' % corpus_bleu(result, predicted, weights=(0, 0, 1, 0)))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"The cumulative and individual 4-gram BLEU use the same weights, e.g. (0, 0, 0, 1)\")\n",
        "# print('Sentence Bleu 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))\n",
        "print('Corpus Bleu 4-gram: %f' % corpus_bleu(result, predicted, weights=(0, 0, 0, 1)))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"The 2-gram weights assign a 50% to each of 1-gram and 2-gram\")\n",
        "print('Corpus Bleu 1-gram: %f' % corpus_bleu(result, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"3-gram weights are 33% for each of the 1, 2 and 3-gram scores\")\n",
        "print('Corpus Bleu 1-gram: %f' % corpus_bleu(result, predicted, weights=(0.33, 0.33, 0.33, 0)))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"4-gram weights are 25% for each of the 1, 2 3 and 4-gram scores\")\n",
        "print('Corpus Bleu 1-gram: %f' % corpus_bleu(result, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print('Corpus Glue Score: %f' % corpus_bleu(result, predicted))\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "totmeteor = 0\n",
        "for sen in range(len(predicted)):\n",
        "  totmeteor+=float(round(meteor_score(result[sen],predicted[sen]),4))\n",
        "print('Meteor Score: ', totmeteor/len(predicted))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "import evaluate\n",
        "rouge = evaluate.load('rouge')\n",
        "roguescore = rouge.compute(predictions=predictedsentences, references=actualsentences)['rougeL']\n",
        "print('Rogue score: ',roguescore)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypix40n4KeK7"
      },
      "source": [
        "# Step 15 - Visualize the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eELgu_AqKeK7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "def generate_caption(image_name):\n",
        "    # load the image\n",
        "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = os.path.join('/content/drive/MyDrive/Mini Project/data/Images/', image_name)\n",
        "    image = Image.open(img_path)\n",
        "    captions = mapping[image_id]\n",
        "    print('---------------------Actual---------------------')\n",
        "    for caption in captions:\n",
        "        print(actual)\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
        "    print('--------------------Predicted--------------------')\n",
        "    print(y_pred)\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTeLxKDiKeK8"
      },
      "outputs": [],
      "source": [
        "generate_caption(\"1001773457_577c3a7d70.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xaa1pn_KeK8"
      },
      "outputs": [],
      "source": [
        "generate_caption(\"1002674143_1b742ab4b8.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0M44-TiKeK-"
      },
      "outputs": [],
      "source": [
        "generate_caption(\"101669240_b2d3e7f17b.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZa0rCFuKeK-"
      },
      "source": [
        "# Product - Test with Real Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S1McAklKeK-"
      },
      "outputs": [],
      "source": [
        "efficientnet_model = EfficientNetB7()\n",
        "# restructure the model\n",
        "efficientnet_model = Model(inputs=efficientnet_model.inputs, outputs=efficientnet_model.layers[-3].output)\n",
        "for layer in efficientnet_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlpUCIJazirf"
      },
      "outputs": [],
      "source": [
        "# to convert the caption to malayalam english to malayalam api\n",
        "\n",
        "!pip install deep-translator\n",
        "from deep_translator import GoogleTranslator\n",
        "translated = GoogleTranslator(source='auto', target='ml').translate(\"keep it up, you are awesome\")\n",
        "\n",
        "def translate_to_malayalam(text):\n",
        "    result = GoogleTranslator(source='en', target='ml').translate(text)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfys_TsfKeK_"
      },
      "outputs": [],
      "source": [
        "# image_path = '/content/drive/MyDrive/Mini Project/data/fortesting.jpeg'\n",
        "image_path = '/content/drive/MyDrive/Mini Project/data/fortest/Copy of 000000581585.jpg'\n",
        "\n",
        "\n",
        "img = Image.open(image_path)\n",
        "plt.imshow(img)\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(600, 600), interpolation='lanczos')\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# Convert the NumPy array to PIL Image\n",
        "pil_img = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "# Apply edge enhancement filter on the image\n",
        "enhanced_image = pil_img.filter(ImageFilter.EDGE_ENHANCE)\n",
        "\n",
        "# Convert the enhanced image back to NumPy array\n",
        "image = img_to_array(enhanced_image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image for vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = efficientnet_model.predict(image, verbose=0)\n",
        "# predict from the trained model\n",
        "pr=predict_caption(model, feature, tokenizer, max_length)\n",
        "# print(pr)\n",
        "pr=pr.split(' ')\n",
        "pr=' '.join(pr[1:-1])\n",
        "print(pr)\n",
        "print(translate_to_malayalam(pr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7tssfVktNo0"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
        "malayalam_caption = {}\n",
        "# Iterate over each image in the folder\n",
        "for i, filename in enumerate(os.listdir('/content/drive/MyDrive/Mini Project/data/fortest')):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check if it's an image file\n",
        "        # Load and process the image\n",
        "        image_path = os.path.join('/content/drive/MyDrive/Mini Project/data/fortest', filename)\n",
        "        img = Image.open(image_path)\n",
        "        image = load_img(image_path, target_size=(600, 600), interpolation='lanczos')\n",
        "        image = img_to_array(image)\n",
        "        pil_img = Image.fromarray(image.astype('uint8'))\n",
        "        enhanced_image = pil_img.filter(ImageFilter.EDGE_ENHANCE)\n",
        "        image = img_to_array(enhanced_image)\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        image = preprocess_input(image)\n",
        "\n",
        "        # Extract features\n",
        "        feature = efficientnet_model.predict(image, verbose=0)\n",
        "\n",
        "        # Predict caption\n",
        "        pr = predict_caption(model, feature, tokenizer, max_length)\n",
        "        pr = pr.split(' ')\n",
        "        pr = ' '.join(pr[1:-1])\n",
        "\n",
        "        # Translate to Malayalam\n",
        "        malayalam_caption[pr] = translate_to_malayalam(pr)\n",
        "\n",
        "        # Display the image with the predicted caption as the title\n",
        "        axs[i // 5, i % 5].imshow(img)\n",
        "        axs[i // 5, i % 5].set_title(pr, fontsize=8)\n",
        "        axs[i // 5, i % 5].axis('off')\n",
        "\n",
        "# Adjust layout to prevent clipping of titles\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtIExK7w8PCN"
      },
      "outputs": [],
      "source": [
        "malayalam_caption"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "quOa5M4owCU7",
        "VJM4RlvywGIj",
        "sfRdgWYUkgtl",
        "SsajfzrdKeKr",
        "3ZP5S46Fxocl",
        "Ih394hMVKeKu",
        "daJQBf2bxz7m",
        "5i34QhMXKeKw",
        "TLLklcvm2Noa",
        "QJHPsSv7xvCy",
        "0Q_Ba3B3KeK3",
        "UgLdOhFa0kdZ",
        "RTXEsGSjKeK4",
        "qB1LPrneKeK6",
        "XwoSxFWgz73P",
        "Ypix40n4KeK7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}